{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests\n",
    "! pip install tensorflow==2.0.0-beta1 --user --ignore-installed\n",
    "! pip install keras\n",
    "! pip install mwparserfromhell\n",
    "! pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # parses HTML\n",
    "from keras.utils import get_file\n",
    "import os # file system management\n",
    "import sys\n",
    "import xml.sax # parsesg xml\n",
    "import re\n",
    "import bz2\n",
    "import subprocess # processes bz2 compressed files line by line\n",
    "import mwparserfromhell # parsing Wiki Code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import json\n",
    "from multiprocessing.dummy import Pool as Threadpool\n",
    "from itertools import chain # chains List of lists to single list\n",
    "from multiprocessing import Pool \n",
    "import tqdm # tracks progress\n",
    "from functools import partial # sends keyword arguments in map\n",
    "from timeit import default_timer as timer\n",
    "from ipynb.fs.full.functions import find_people\n",
    "from json import JSONEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access a dump (snapshot) of the entire Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "index = requests.get(base_url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BeautifulSoup to create a parse tree that can be used to extract data from HTML, and find the links on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href')]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a complete dump and find all available files in the dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = base_url + '20190701/'\n",
    "# Retrieve the html\n",
    "dump_html = requests.get(dump_url).text\n",
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "# Find list elements with the class file\n",
    "soup_dump.find_all('li', {'class': 'file'})[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit to only the files containing 'pages-articles' to get only the recent versions of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "# Search through all files\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    # Select the relevant files\n",
    "    if 'pages-articles-multistream' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "        \n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further filter for partitioned files with 'xml-p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = [file[0] for file in files if '.xml-p' in file[0]]\n",
    "files_to_download[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if file already exists and if not download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_home = 'C:\\\\Users\\\\efan\\\\.keras\\\\datasets\\\\'\n",
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files_to_download:\n",
    "    path = keras_home + file\n",
    "    \n",
    "    # Check to see if the path exists (if the file is already downloaded)\n",
    "    if not os.path.exists(keras_home + file):\n",
    "        print('Downloading')\n",
    "        # If not, download the file\n",
    "        data_paths.append(get_file(file, dump_url + file))\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # If the file is already downloaded find some information\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by file size to see the largest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(file_info, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many partitions there are in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(file_info)} partitions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this info into a dataframe to plot the file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "file_df = pd.DataFrame(file_info, columns = ['file', 'size (MB)', 'articles']).set_index('file')\n",
    "file_df['size (MB)'].plot.bar(color = 'red', figsize = (12, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the total file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total size of files on disk is {file_df['size (MB)'].sum() / 1e3} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Step 1: Extract info from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(title, text, timestamp, template = 'Infobox person'):\n",
    "    \"\"\"Process a wikipedia article looking for template\"\"\"\n",
    "    \n",
    "    # Create a parsing object\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    \n",
    "    # Search through templates for the template\n",
    "    matches = wikicode.filter_templates(matches = template)\n",
    "    \n",
    "    # Filter out errant matches\n",
    "    matches = [x for x in matches if x.name.strip_code().strip().lower() == template.lower()]\n",
    "    \n",
    "    if len(matches) >= 1:\n",
    "        # Extract unnested information from infobox\n",
    "        properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                      for param in matches[0].params\n",
    "                      if param.value.strip_code().strip()}\n",
    "        \n",
    "        # Extract Birth Date\n",
    "        try:\n",
    "            birth_date1 = wikicode.filter_templates(matches = 'birth date')[-1].params\n",
    "            birth_date = [str(x) for x in birth_date1 if not '=' in x]\n",
    "        except:\n",
    "            birth_date = 'unknown'\n",
    "            \n",
    "        # Extract spouse\n",
    "        try:\n",
    "            spouse1 = wikicode.filter_templates(matches = 'marriage') + wikicode.filter_templates(matches = 'married')\n",
    "            spouse2 = [x.params for x in spouse1 if x.name.lower() == 'marriage' or x.name.lower() == 'married']\n",
    "            spouse = [list(str(y) for y in x) for x in spouse2]\n",
    "            nested = 'nested'\n",
    "        except:\n",
    "            spouse = [str(properties.get('spouse'))] + re.findall(r'\\((.*?)\\)', str(properties.get('spouse')))\n",
    "            nested = 'not_nested'\n",
    "        \n",
    "        # Extract internal wikilinks - filter for categories they belong to\n",
    "        wikilinks1 = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "        wikilinks = [str(x) for x in wikilinks1 if 'Category:' in x]\n",
    "        label = [x for x in wikilinks if '21st-century' in x and 'act' in x]\n",
    "        \n",
    "        # Extract gender and nationality\n",
    "        if 'actresses' in str(label):\n",
    "            gender = 'female'\n",
    "            try:\n",
    "                nationality = [re.match('Category:21st-century (.*) actresses',x).group(1) for x in label]\n",
    "            except:\n",
    "                nationality = 'unknown'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "            try:\n",
    "                nationality = [re.match('Category:21st-century (.*) male actors',x).group(1) for x in label]\n",
    "            except:\n",
    "                nationality = 'unknown'\n",
    "                \n",
    "        # Extract nested and unnested awards\n",
    "        awards1 = wikicode.filter_templates(matches ='awards')\n",
    "        awards2 = [str(x.params[0]) for x in awards1 if x.name.lower()=='awards']\n",
    "        awards3 = [str(x) for x in wikilinks if 'winner' in x]\n",
    "        awards = awards2+awards3\n",
    "        \n",
    "        return (title, birth_date, spouse, nested, gender, nationality, awards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing step 2: set up XML Handler to parse XML using SAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Parse through XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._people = []\n",
    "        self._article_count = 0\n",
    "        self._non_matches = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page' and (re.search('21st\\-century\\s+(?:(?!\\s+actors\")(?:.|\\n))*\\s+actors', str(self._buffer))\n",
    "                               or\n",
    "                               re.search('21st\\-century\\s+(?:(?!\\s+actors\")(?:.|\\n))*\\s+actresses', str(self._buffer))):\n",
    "            self._article_count += 1\n",
    "            # Search through the page to see if the page is a person\n",
    "            people = process_article(**self._values, template = 'Infobox person')\n",
    "            # Append to the list of people\n",
    "            if people:\n",
    "                self._people.append(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Step 3: Set up parser to decompress one line from compressed file at a time, pass to XML Handler to process and write to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_people(data_path, limit = None, save = True):\n",
    "    \"\"\"Find all the people from a compressed wikipedia XML dump.\n",
    "       `limit` could limit to only return a set number of books.\n",
    "        If save, books are saved to partition directory based on file name\"\"\"\n",
    "\n",
    "    # Object for handling xml\n",
    "    handler = WikiXmlHandler()\n",
    "\n",
    "    # Parsing object\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    # Iterate through compressed file\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data_path), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        # Optional limit\n",
    "        if limit is not None and len(handler._people) >= limit:\n",
    "            return handler._people\n",
    "    \n",
    "    if save:\n",
    "        partition_dir = 'C:\\\\Users\\\\efan\\\\wiki\\\\partitions\\\\'\n",
    "        # Create file name based on partition name\n",
    "        p_str = data_path.split('-')[-1].split('.')[-2]\n",
    "        out_dir = partition_dir + f'{p_str}.ndjson'\n",
    "\n",
    "        # Open the file\n",
    "        with open(out_dir, 'w') as fout:\n",
    "            # Write as json\n",
    "            for person in handler._people:\n",
    "                fout.write(json.dumps(person) + '\\n')\n",
    "        \n",
    "        print(f'{len(os.listdir(partition_dir))} files processed.', end = '\\r')\n",
    "\n",
    "    # Memory management\n",
    "    del handler\n",
    "    del parser\n",
    "    gc.collect()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [keras_home + file for file in os.listdir(keras_home) if 'xml-p' in file]\n",
    "len(partitions), partitions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a CPU Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process all the partitions with a progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pool of workers to execute processes\n",
    "pool = Pool(processes = 4)\n",
    "\n",
    "start = timer()\n",
    "results = []\n",
    "\n",
    "# Map (service, tasks), applies function to each partition\n",
    "for x in tqdm.tqdm_notebook(pool.imap_unordered(find_people, partitions), total = len(partitions)):\n",
    "    results.append(x)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = timer()\n",
    "print(f'{end - start} seconds elapsed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
